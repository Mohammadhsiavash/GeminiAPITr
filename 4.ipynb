{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Chat Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=os.environ.get('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatSession(\n",
       "    model=genai.GenerativeModel(\n",
       "        model_name='models/learnlm-1.5-pro-experimental',\n",
       "        generation_config={},\n",
       "        safety_settings={},\n",
       "        tools=None,\n",
       "        system_instruction=None,\n",
       "        cached_content=None\n",
       "    ),\n",
       "    history=[]\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel('learnlm-1.5-pro-experimental')\n",
    "chat = model.start_chat(history=[])\n",
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"There isn't a single, universally defined set of \\\"Gemini models.\\\"  \\\"Gemini\\\" refers to a family of large language models (LLMs) developed by Google AI.  Different Gemini models are designed for different tasks and capabilities.  Think of it like a family with members who have different skills and strengths.  While we don't have exhaustive public details on every Gemini model variation, here's a breakdown of the key differentiating factors:\\n\\n**Key Differences Between Gemini Models:**\\n\\n* **Size and Capabilities:**  Like most LLMs, Gemini models likely come in various sizes, impacting their performance on tasks requiring complex reasoning, knowledge recall, and nuanced language understanding. Larger models generally perform better but require more computational resources.  We know of:\\n    * **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks.  It excels at reasoning, math, and programming.  Consider this the \\\"expert\\\" of the family.\\n    * **Gemini Pro:** A powerful model suitable for a wide range of professional applications, offering a balance between performance and efficiency.  This could be the \\\"all-rounder.\\\"\\n    * **Gemini Nano:**  The most efficient model, designed for on-device tasks and applications requiring lower latency. Think of this as the \\\"quick and nimble\\\" member.\\n\\n* **Modality (Multimodal vs. Text-Only):** A crucial distinction lies in a model's ability to handle different types of input and output:\\n    * **Multimodal:** Some Gemini models are multimodal, meaning they can process and understand information beyond text, including images, audio, and video.  This allows for more sophisticated interactions and applications.  Gemini Ultra is confirmed to be multimodal.\\n    * **Text-Only:** Other Gemini models might focus solely on text-based input and output.  These are excellent for tasks like writing, translation, and code generation where other modalities aren't essential.\\n\\n* **Training Data and Specialization:**  While the core training data might be similar, specific Gemini models could be fine-tuned on specialized datasets for particular domains, such as:\\n    * **Code Generation:** Models specifically trained on code could excel at programming tasks.\\n    * **Scientific Research:** Models trained on scientific literature could assist with research and analysis.\\n    * **Medical Applications:** Models trained on medical data could aid in diagnosis and treatment planning.\\n\\n* **Deployment and Accessibility:** Different Gemini models are designed for different deployment scenarios:\\n    * **Cloud-Based:** Larger models like Gemini Ultra are likely accessed via cloud APIs due to their computational demands.\\n    * **On-Device:** Smaller, more efficient models like Gemini Nano are optimized to run directly on devices like smartphones or embedded systems.  This enables offline functionality and reduced latency.\\n\\n\\n**In Summary:**\\n\\nThe Gemini family comprises a diverse set of models tailored to a spectrum of needs and applications.  Differences in size, capabilities, modality, specialization, and deployment options allow developers and users to choose the model best suited for their specific requirements.  As Google releases more information, the distinctions between these models will become clearer.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0,\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 8,\n",
      "        \"candidates_token_count\": 639,\n",
      "        \"total_token_count\": 647\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('explain gemini models differnces')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parts {\n",
       "  text: \"explain gemini models differnces\"\n",
       "}\n",
       "role: \"user\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parts {\n",
       "  text: \"There isn\\'t a single, universally defined set of \\\"Gemini models.\\\"  \\\"Gemini\\\" refers to a family of large language models (LLMs) developed by Google AI.  Different Gemini models are designed for different tasks and capabilities.  Think of it like a family with members who have different skills and strengths.  While we don\\'t have exhaustive public details on every Gemini model variation, here\\'s a breakdown of the key differentiating factors:\\n\\n**Key Differences Between Gemini Models:**\\n\\n* **Size and Capabilities:**  Like most LLMs, Gemini models likely come in various sizes, impacting their performance on tasks requiring complex reasoning, knowledge recall, and nuanced language understanding. Larger models generally perform better but require more computational resources.  We know of:\\n    * **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks.  It excels at reasoning, math, and programming.  Consider this the \\\"expert\\\" of the family.\\n    * **Gemini Pro:** A powerful model suitable for a wide range of professional applications, offering a balance between performance and efficiency.  This could be the \\\"all-rounder.\\\"\\n    * **Gemini Nano:**  The most efficient model, designed for on-device tasks and applications requiring lower latency. Think of this as the \\\"quick and nimble\\\" member.\\n\\n* **Modality (Multimodal vs. Text-Only):** A crucial distinction lies in a model\\'s ability to handle different types of input and output:\\n    * **Multimodal:** Some Gemini models are multimodal, meaning they can process and understand information beyond text, including images, audio, and video.  This allows for more sophisticated interactions and applications.  Gemini Ultra is confirmed to be multimodal.\\n    * **Text-Only:** Other Gemini models might focus solely on text-based input and output.  These are excellent for tasks like writing, translation, and code generation where other modalities aren\\'t essential.\\n\\n* **Training Data and Specialization:**  While the core training data might be similar, specific Gemini models could be fine-tuned on specialized datasets for particular domains, such as:\\n    * **Code Generation:** Models specifically trained on code could excel at programming tasks.\\n    * **Scientific Research:** Models trained on scientific literature could assist with research and analysis.\\n    * **Medical Applications:** Models trained on medical data could aid in diagnosis and treatment planning.\\n\\n* **Deployment and Accessibility:** Different Gemini models are designed for different deployment scenarios:\\n    * **Cloud-Based:** Larger models like Gemini Ultra are likely accessed via cloud APIs due to their computational demands.\\n    * **On-Device:** Smaller, more efficient models like Gemini Nano are optimized to run directly on devices like smartphones or embedded systems.  This enables offline functionality and reduced latency.\\n\\n\\n**In Summary:**\\n\\nThe Gemini family comprises a diverse set of models tailored to a spectrum of needs and applications.  Differences in size, capabilities, modality, specialization, and deployment options allow developers and users to choose the model best suited for their specific requirements.  As Google releases more information, the distinctions between these models will become clearer.\\n\"\n",
       "}\n",
       "role: \"model\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There isn\\'t a single, universally defined set of \"Gemini models.\"  \"Gemini\" refers to a family of large language models (LLMs) developed by Google AI.  Different Gemini models are designed for different tasks and capabilities.  Think of it like a family with members who have different skills and strengths.  While we don\\'t have exhaustive public details on every Gemini model variation, here\\'s a breakdown of the key differentiating factors:\\n\\n**Key Differences Between Gemini Models:**\\n\\n* **Size and Capabilities:**  Like most LLMs, Gemini models likely come in various sizes, impacting their performance on tasks requiring complex reasoning, knowledge recall, and nuanced language understanding. Larger models generally perform better but require more computational resources.  We know of:\\n    * **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks.  It excels at reasoning, math, and programming.  Consider this the \"expert\" of the family.\\n    * **Gemini Pro:** A powerful model suitable for a wide range of professional applications, offering a balance between performance and efficiency.  This could be the \"all-rounder.\"\\n    * **Gemini Nano:**  The most efficient model, designed for on-device tasks and applications requiring lower latency. Think of this as the \"quick and nimble\" member.\\n\\n* **Modality (Multimodal vs. Text-Only):** A crucial distinction lies in a model\\'s ability to handle different types of input and output:\\n    * **Multimodal:** Some Gemini models are multimodal, meaning they can process and understand information beyond text, including images, audio, and video.  This allows for more sophisticated interactions and applications.  Gemini Ultra is confirmed to be multimodal.\\n    * **Text-Only:** Other Gemini models might focus solely on text-based input and output.  These are excellent for tasks like writing, translation, and code generation where other modalities aren\\'t essential.\\n\\n* **Training Data and Specialization:**  While the core training data might be similar, specific Gemini models could be fine-tuned on specialized datasets for particular domains, such as:\\n    * **Code Generation:** Models specifically trained on code could excel at programming tasks.\\n    * **Scientific Research:** Models trained on scientific literature could assist with research and analysis.\\n    * **Medical Applications:** Models trained on medical data could aid in diagnosis and treatment planning.\\n\\n* **Deployment and Accessibility:** Different Gemini models are designed for different deployment scenarios:\\n    * **Cloud-Based:** Larger models like Gemini Ultra are likely accessed via cloud APIs due to their computational demands.\\n    * **On-Device:** Smaller, more efficient models like Gemini Nano are optimized to run directly on devices like smartphones or embedded systems.  This enables offline functionality and reduced latency.\\n\\n\\n**In Summary:**\\n\\nThe Gemini family comprises a diverse set of models tailored to a spectrum of needs and applications.  Differences in size, capabilities, modality, specialization, and deployment options allow developers and users to choose the model best suited for their specific requirements.  As Google releases more information, the distinctions between these models will become clearer.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history[1].parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=protos.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"text\": \"While precise details on all Gemini models' input types aren't fully public, here's a general outline based on available information:\\n\\n* **Gemini Ultra:** Confirmed **multimodal**. Accepts **text, images, audio, and potentially video** as input.  This allows for complex queries combining different data types (e.g., \\\"describe what's happening in this image and write a short story about it\\\").\\n\\n* **Gemini Pro:** Likely primarily **text-based** input, but there's a possibility of limited multimodal capabilities depending on the specific variant.  More information is needed from Google for confirmation.\\n\\n* **Gemini Nano:** Designed for **on-device** use, primarily focused on **text input** due to resource constraints.  Multimodal capabilities are less likely at this scale.  Optimized for quick text-based interactions.\\n\"\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": \"STOP\",\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            },\n",
       "            {\n",
       "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
       "              \"probability\": \"NEGLIGIBLE\"\n",
       "            }\n",
       "          ]\n",
       "        }\n",
       "      ],\n",
       "      \"usage_metadata\": {\n",
       "        \"prompt_token_count\": 661,\n",
       "        \"candidates_token_count\": 174,\n",
       "        \"total_token_count\": 835\n",
       "      }\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chat.send_message('ok, now explain shortly about data input types of each')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"explain gemini models differnces\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"There isn\\'t a single, universally defined set of \\\"Gemini models.\\\"  \\\"Gemini\\\" refers to a family of large language models (LLMs) developed by Google AI.  Different Gemini models are designed for different tasks and capabilities.  Think of it like a family with members who have different skills and strengths.  While we don\\'t have exhaustive public details on every Gemini model variation, here\\'s a breakdown of the key differentiating factors:\\n\\n**Key Differences Between Gemini Models:**\\n\\n* **Size and Capabilities:**  Like most LLMs, Gemini models likely come in various sizes, impacting their performance on tasks requiring complex reasoning, knowledge recall, and nuanced language understanding. Larger models generally perform better but require more computational resources.  We know of:\\n    * **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks.  It excels at reasoning, math, and programming.  Consider this the \\\"expert\\\" of the family.\\n    * **Gemini Pro:** A powerful model suitable for a wide range of professional applications, offering a balance between performance and efficiency.  This could be the \\\"all-rounder.\\\"\\n    * **Gemini Nano:**  The most efficient model, designed for on-device tasks and applications requiring lower latency. Think of this as the \\\"quick and nimble\\\" member.\\n\\n* **Modality (Multimodal vs. Text-Only):** A crucial distinction lies in a model\\'s ability to handle different types of input and output:\\n    * **Multimodal:** Some Gemini models are multimodal, meaning they can process and understand information beyond text, including images, audio, and video.  This allows for more sophisticated interactions and applications.  Gemini Ultra is confirmed to be multimodal.\\n    * **Text-Only:** Other Gemini models might focus solely on text-based input and output.  These are excellent for tasks like writing, translation, and code generation where other modalities aren\\'t essential.\\n\\n* **Training Data and Specialization:**  While the core training data might be similar, specific Gemini models could be fine-tuned on specialized datasets for particular domains, such as:\\n    * **Code Generation:** Models specifically trained on code could excel at programming tasks.\\n    * **Scientific Research:** Models trained on scientific literature could assist with research and analysis.\\n    * **Medical Applications:** Models trained on medical data could aid in diagnosis and treatment planning.\\n\\n* **Deployment and Accessibility:** Different Gemini models are designed for different deployment scenarios:\\n    * **Cloud-Based:** Larger models like Gemini Ultra are likely accessed via cloud APIs due to their computational demands.\\n    * **On-Device:** Smaller, more efficient models like Gemini Nano are optimized to run directly on devices like smartphones or embedded systems.  This enables offline functionality and reduced latency.\\n\\n\\n**In Summary:**\\n\\nThe Gemini family comprises a diverse set of models tailored to a spectrum of needs and applications.  Differences in size, capabilities, modality, specialization, and deployment options allow developers and users to choose the model best suited for their specific requirements.  As Google releases more information, the distinctions between these models will become clearer.\\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"ok, now explain shortly about data input types of each\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"While precise details on all Gemini models\\' input types aren\\'t fully public, here\\'s a general outline based on available information:\\n\\n* **Gemini Ultra:** Confirmed **multimodal**. Accepts **text, images, audio, and potentially video** as input.  This allows for complex queries combining different data types (e.g., \\\"describe what\\'s happening in this image and write a short story about it\\\").\\n\\n* **Gemini Pro:** Likely primarily **text-based** input, but there\\'s a possibility of limited multimodal capabilities depending on the specific variant.  More information is needed from Google for confirmation.\\n\\n* **Gemini Nano:** Designed for **on-device** use, primarily focused on **text input** due to resource constraints.  Multimodal capabilities are less likely at this scale.  Optimized for quick text-based interactions.\\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While precise details on all Gemini models\\' input types aren\\'t fully public, here\\'s a general outline based on available information:\\n\\n* **Gemini Ultra:** Confirmed **multimodal**. Accepts **text, images, audio, and potentially video** as input.  This allows for complex queries combining different data types (e.g., \"describe what\\'s happening in this image and write a short story about it\").\\n\\n* **Gemini Pro:** Likely primarily **text-based** input, but there\\'s a possibility of limited multimodal capabilities depending on the specific variant.  More information is needed from Google for confirmation.\\n\\n* **Gemini Nano:** Designed for **on-device** use, primarily focused on **text input** due to resource constraints.  Multimodal capabilities are less likely at this scale.  Optimized for quick text-based interactions.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history[3].parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message('is there any else multimmodal model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes, besides Gemini, there are several other prominent multimodal models. Here are a few examples:\\n\\n* **GPT-4:** OpenAI\\'s flagship model, GPT-4, is multimodal, accepting both image and text inputs.  It can caption images, answer questions about them, and even generate creative text formats,  like poems, code, scripts, musical pieces, email, etc., based on visual input.\\n\\n* **Flamingo:** Developed by DeepMind, Flamingo is a visual language model capable of understanding and reasoning about the world by combining visual information with language.  It excels at tasks like visual question answering and image captioning.\\n\\n* **BLIP-2:**  Short for \"Bootstrapping Language-Image Pre-training,\" BLIP-2 is a model designed for efficient and robust image understanding. It can perform tasks like visual question answering, image captioning, and image-text retrieval.  It\\'s known for its strong performance despite being relatively computationally efficient.\\n\\n* **Kosmos-1:**  Developed by Microsoft, Kosmos-1 is a multimodal large language model (MLLM) that can perceive general modalities, follow instructions, and perform in-context learning. It can analyze images, answer questions about them, write stories based on visual input and more.  Microsoft positions it as a step towards more general-purpose AI models.\\n\\n* **VisualBERT:** One of the earlier influential multimodal models, VisualBERT is designed to jointly understand text and image data.  It\\'s often used for tasks like visual question answering and image captioning.\\n\\n\\n**Key Considerations:**\\n\\n* **Accessibility:**  Not all of these models are publicly available. Some are accessible through APIs or research collaborations, while others might be limited to internal use within their respective organizations.\\n* **Capabilities:** Each model has its strengths and weaknesses in terms of the types of multimodal tasks it can perform and the complexity of the information it can process.\\n* **Evolving Field:** The field of multimodal AI is rapidly evolving, with new models and architectures being developed continuously. This list represents a snapshot of prominent models at this time but is not exhaustive.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.history[5].parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Llama\" refers to the **Large Language Model Meta AI** developed by Meta.  There are indeed different versions of Llama, each with varying capabilities and designed for different purposes. Here\\'s a breakdown:\\n\\n**Key Differences Between Llama Versions:**\\n\\n* **Size and Performance:** Llama models, like other LLMs, come in different sizes, measured by the number of parameters.  More parameters generally correlate with higher performance on tasks requiring complex reasoning and knowledge:\\n    * **Llama 2:** 7B, 13B, and 70B parameter versions available. The larger the model, the better its performance, but the more computational resources it requires.  70B parameter version is considered to be the most powerful\\n    * **Llama 1:** 7B, 13B, 33B, and 65B parameter versions. The range is similar to Llama 2, but Llama 2 models of the same size generally outperform Llama 1.  Llama 1 was not released to the public, only Llama 2.\\n\\n* **Fine-tuning and Specialization:** While the base Llama models provide a foundation, they can be further fine-tuned for specific tasks or domains:\\n    * **Chatbots:** Fine-tuning on conversational data creates models optimized for interactive dialogues.\\n    * **Code Generation:**  Training on code datasets enables models to generate and understand programming languages.\\n    * **Translation:** Fine-tuning on multilingual text allows models to perform translation tasks.\\n\\n\\n* **Licensing and Access:** A significant difference lies in how these models are made available:\\n    * **Llama 2:** Available for commercial use. It is intended to give businesses, startups, entrepreneurs, and researchers access to tools developed at a scale that would be challenging to build themselves, backed by computing power they might not otherwise have access to.  Meta believes that an open approach is the right one for the development of today’s AI models, especially those in the generative space where the technology is rapidly advancing.\\n    * **Llama 1:** **Not** for Commercial Use; Research Focused. It was released under a noncommercial license focused on research use cases.  Access was granted on a case-by-case basis to researchers.\\n\\n**In Summary:**\\n\\nThe different Llama versions provide options for various needs and resource constraints.  Researchers and developers can select the model size and fine-tuning that best suits their specific applications. The evolution from Llama 1 to Llama 2 demonstrates a shift towards broader accessibility and commercial applicability, reflecting the dynamic nature of the LLM landscape.  The move to an open-source model by Meta is meant to foster innovation and collaboration within the AI community.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.send_message('what  about different lama vesions?')\n",
    "chat.history[7].parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: explain gemini models differnces\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: There isn't a single, universally defined set of \"Gemini models.\"  \"Gemini\" refers to a family of large language models (LLMs) developed by Google AI.  Different Gemini models are designed for different tasks and capabilities.  Think of it like a family with members who have different skills and strengths.  While we don't have exhaustive public details on every Gemini model variation, here's a breakdown of the key differentiating factors:\n",
      "\n",
      "**Key Differences Between Gemini Models:**\n",
      "\n",
      "* **Size and Capabilities:**  Like most LLMs, Gemini models likely come in various sizes, impacting their performance on tasks requiring complex reasoning, knowledge recall, and nuanced language understanding. Larger models generally perform better but require more computational resources.  We know of:\n",
      "    * **Gemini Ultra:** The largest and most capable model, designed for highly complex tasks.  It excels at reasoning, math, and programming.  Consider this the \"expert\" of the family.\n",
      "    * **Gemini Pro:** A powerful model suitable for a wide range of professional applications, offering a balance between performance and efficiency.  This could be the \"all-rounder.\"\n",
      "    * **Gemini Nano:**  The most efficient model, designed for on-device tasks and applications requiring lower latency. Think of this as the \"quick and nimble\" member.\n",
      "\n",
      "* **Modality (Multimodal vs. Text-Only):** A crucial distinction lies in a model's ability to handle different types of input and output:\n",
      "    * **Multimodal:** Some Gemini models are multimodal, meaning they can process and understand information beyond text, including images, audio, and video.  This allows for more sophisticated interactions and applications.  Gemini Ultra is confirmed to be multimodal.\n",
      "    * **Text-Only:** Other Gemini models might focus solely on text-based input and output.  These are excellent for tasks like writing, translation, and code generation where other modalities aren't essential.\n",
      "\n",
      "* **Training Data and Specialization:**  While the core training data might be similar, specific Gemini models could be fine-tuned on specialized datasets for particular domains, such as:\n",
      "    * **Code Generation:** Models specifically trained on code could excel at programming tasks.\n",
      "    * **Scientific Research:** Models trained on scientific literature could assist with research and analysis.\n",
      "    * **Medical Applications:** Models trained on medical data could aid in diagnosis and treatment planning.\n",
      "\n",
      "* **Deployment and Accessibility:** Different Gemini models are designed for different deployment scenarios:\n",
      "    * **Cloud-Based:** Larger models like Gemini Ultra are likely accessed via cloud APIs due to their computational demands.\n",
      "    * **On-Device:** Smaller, more efficient models like Gemini Nano are optimized to run directly on devices like smartphones or embedded systems.  This enables offline functionality and reduced latency.\n",
      "\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "The Gemini family comprises a diverse set of models tailored to a spectrum of needs and applications.  Differences in size, capabilities, modality, specialization, and deployment options allow developers and users to choose the model best suited for their specific requirements.  As Google releases more information, the distinctions between these models will become clearer.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: ok, now explain shortly about data input types of each\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: While precise details on all Gemini models' input types aren't fully public, here's a general outline based on available information:\n",
      "\n",
      "* **Gemini Ultra:** Confirmed **multimodal**. Accepts **text, images, audio, and potentially video** as input.  This allows for complex queries combining different data types (e.g., \"describe what's happening in this image and write a short story about it\").\n",
      "\n",
      "* **Gemini Pro:** Likely primarily **text-based** input, but there's a possibility of limited multimodal capabilities depending on the specific variant.  More information is needed from Google for confirmation.\n",
      "\n",
      "* **Gemini Nano:** Designed for **on-device** use, primarily focused on **text input** due to resource constraints.  Multimodal capabilities are less likely at this scale.  Optimized for quick text-based interactions.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: is there any else multimmodal model?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: Yes, besides Gemini, there are several other prominent multimodal models. Here are a few examples:\n",
      "\n",
      "* **GPT-4:** OpenAI's flagship model, GPT-4, is multimodal, accepting both image and text inputs.  It can caption images, answer questions about them, and even generate creative text formats,  like poems, code, scripts, musical pieces, email, etc., based on visual input.\n",
      "\n",
      "* **Flamingo:** Developed by DeepMind, Flamingo is a visual language model capable of understanding and reasoning about the world by combining visual information with language.  It excels at tasks like visual question answering and image captioning.\n",
      "\n",
      "* **BLIP-2:**  Short for \"Bootstrapping Language-Image Pre-training,\" BLIP-2 is a model designed for efficient and robust image understanding. It can perform tasks like visual question answering, image captioning, and image-text retrieval.  It's known for its strong performance despite being relatively computationally efficient.\n",
      "\n",
      "* **Kosmos-1:**  Developed by Microsoft, Kosmos-1 is a multimodal large language model (MLLM) that can perceive general modalities, follow instructions, and perform in-context learning. It can analyze images, answer questions about them, write stories based on visual input and more.  Microsoft positions it as a step towards more general-purpose AI models.\n",
      "\n",
      "* **VisualBERT:** One of the earlier influential multimodal models, VisualBERT is designed to jointly understand text and image data.  It's often used for tasks like visual question answering and image captioning.\n",
      "\n",
      "\n",
      "**Key Considerations:**\n",
      "\n",
      "* **Accessibility:**  Not all of these models are publicly available. Some are accessible through APIs or research collaborations, while others might be limited to internal use within their respective organizations.\n",
      "* **Capabilities:** Each model has its strengths and weaknesses in terms of the types of multimodal tasks it can perform and the complexity of the information it can process.\n",
      "* **Evolving Field:** The field of multimodal AI is rapidly evolving, with new models and architectures being developed continuously. This list represents a snapshot of prominent models at this time but is not exhaustive.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "User: what  about different lama vesions?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Model: \"Llama\" refers to the **Large Language Model Meta AI** developed by Meta.  There are indeed different versions of Llama, each with varying capabilities and designed for different purposes. Here's a breakdown:\n",
      "\n",
      "**Key Differences Between Llama Versions:**\n",
      "\n",
      "* **Size and Performance:** Llama models, like other LLMs, come in different sizes, measured by the number of parameters.  More parameters generally correlate with higher performance on tasks requiring complex reasoning and knowledge:\n",
      "    * **Llama 2:** 7B, 13B, and 70B parameter versions available. The larger the model, the better its performance, but the more computational resources it requires.  70B parameter version is considered to be the most powerful\n",
      "    * **Llama 1:** 7B, 13B, 33B, and 65B parameter versions. The range is similar to Llama 2, but Llama 2 models of the same size generally outperform Llama 1.  Llama 1 was not released to the public, only Llama 2.\n",
      "\n",
      "* **Fine-tuning and Specialization:** While the base Llama models provide a foundation, they can be further fine-tuned for specific tasks or domains:\n",
      "    * **Chatbots:** Fine-tuning on conversational data creates models optimized for interactive dialogues.\n",
      "    * **Code Generation:**  Training on code datasets enables models to generate and understand programming languages.\n",
      "    * **Translation:** Fine-tuning on multilingual text allows models to perform translation tasks.\n",
      "\n",
      "\n",
      "* **Licensing and Access:** A significant difference lies in how these models are made available:\n",
      "    * **Llama 2:** Available for commercial use. It is intended to give businesses, startups, entrepreneurs, and researchers access to tools developed at a scale that would be challenging to build themselves, backed by computing power they might not otherwise have access to.  Meta believes that an open approach is the right one for the development of today’s AI models, especially those in the generative space where the technology is rapidly advancing.\n",
      "    * **Llama 1:** **Not** for Commercial Use; Research Focused. It was released under a noncommercial license focused on research use cases.  Access was granted on a case-by-case basis to researchers.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "The different Llama versions provide options for various needs and resource constraints.  Researchers and developers can select the model size and fine-tuning that best suits their specific applications. The evolution from Llama 1 to Llama 2 demonstrates a shift towards broader accessibility and commercial applicability, reflecting the dynamic nature of the LLM landscape.  The move to an open-source model by Meta is meant to foster innovation and collaboration within the AI community.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item  in chat.history:\n",
    "    print(f'{item.role.capitalize()}: {item.parts[0].text}')\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
